{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run insta_feature_engineering.py\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import roc_curve, auc, f1_score, classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# enable garbage collector to aid in memory \n",
    "gc.enable()\n",
    "# eliminate future warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean encoding of aisle and department\n",
    "aisle_mean = (df_train.groupby('aisle_id')['reordered'].mean()).to_frame('aisle').reset_index()\n",
    "department_mean = (df_train.groupby('department_id')['reordered'].mean()).to_frame('department').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging new features on test set\n",
    "df_test = df_test.merge(aisle_mean, on = 'aisle_id', how = 'left')\n",
    "df_test = df_test.merge(department_mean, on = 'department_id', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging new features on training set\n",
    "df_train = df_train.merge(aisle_mean, on = 'aisle_id', how = 'left')\n",
    "df_train = df_train.merge(department_mean, on = 'department_id', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no longer need the actual id's in test or train set\n",
    "df_test.drop(['aisle_id', 'department_id'], axis = 1, inplace = True)\n",
    "df_train.drop(['aisle_id', 'department_id'], axis = 1, inplace = True)\n",
    "df_test.set_index(['user_id', 'product_id'], inplace = True)\n",
    "df_train.set_index(['user_id', 'product_id'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensuring columns are the same, train has an extra due to reordered column \n",
    "df_test.shape, df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del aisle_mean, department_mean\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking covariance on the features and will run both models with and without \n",
    "# last_five_up and ratio_last_five_up as it appears covariant with other features\n",
    "sns.pairplot(df_train.head(10_000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# X and y Variables for models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These will be used as the base for X and y variables throughout the notebook\n",
    "X, y = df_train.drop('reordered', axis=1), df_train.reordered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_train\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balancing the target class SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# balance data for better results from sampling \n",
    "smote = SMOTE(random_state = 13)\n",
    "smote_X, smote_y = smote.fit_sample(X, y)\n",
    "smote_X = pd.DataFrame(smote_X, columns = X.columns )\n",
    "smote_y= pd.DataFrame(smote_y, columns=['reordered'])\n",
    "# we can Check the numbers of our data\n",
    "print(\"length is\",len(smote_X))\n",
    "print(\"Number of  not reordered\",len(smote_y[smote_y['reordered'] == 0]))\n",
    "print(\"Number of reordered\",len(smote_y[smote_y['reordered'] == 1]))\n",
    "print(\"Proportion of not reordered \", len(smote_y[smote_y['reordered'] == 0]) / len(smote_X))\n",
    "print(\"Proportion of reordered\", len(smote_y[smote_y['reordered'] == 1]) / len(smote_X))\n",
    "# class is balanced with equal proportions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting for cross validation\n",
    "X_train, X_test, y_train, y_test = train_test_split(smote_X, smote_y, test_size = .2, random_state = 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard scaler used to keep variation\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data scaled for better results and convergence\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used to check feature importance again and coefficients\n",
    "import statsmodels.api as sm\n",
    "logit_model=sm.Logit(y_train, X_train)\n",
    "result=logit_model.fit(maxiter = 200)\n",
    "print(result.summary2())\n",
    "# statsmodel failed to converge due to covariance issue with both features, confirms deleting\n",
    "# results summary shows last_five and last_five ratio are to be rejected as the p value is greater than 5%\n",
    "# most_hour just meets the threshold and will fail to be rejected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate model for grid search\n",
    "lr1 = LogisticRegression(random_state = 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary of different hyperparameters to use during the gridsearch\n",
    "params = {'C': [100_000, 1_000_000, 10_000_000], \n",
    "       'penalty': ['l1', 'l2'], \n",
    "       }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate gridsearch to run\n",
    "lr1_cv = GridSearchCV(lr1, params, cv = 5, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runs the search to extract the best hyperparamters to use\n",
    "lr1_cv.fit(X_train, np.ravel(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prints the best parameters, will decide based on given best parameters\n",
    "print(\"tuned hpyerparameters :(best parameters) \", lr1_cv.best_params_)\n",
    "print(\"accuracy :\", lr1_cv.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run tuned Logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping insignificant features who were found to have zero importance and covariance\n",
    "important_X = smote_X.drop(['last_five_up', 'ratio_last_five_up'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting for cross validation with dropped columns\n",
    "X_train, X_test, y_train, y_test = train_test_split(important_X, smote_y, test_size = .2, random_state = 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data scaled for better results and convergence\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate a newly configured model\n",
    "tune_lr = LogisticRegression(random_state = 13, class_weight = 'balanced', C = 1_000_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the newly configured model\n",
    "tune_lr.fit(X_train, np.ravel(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions made to validate\n",
    "tune_pred = tune_lr.predict(X_test)\n",
    "tune_prob = tune_lr.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensuring a good distribution \n",
    "tune_prob.min(), tune_prob.max(), tune_prob.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking with histogram for a good distribution of log odds\n",
    "plt.hist(tune_prob);\n",
    "plt.title('Probability Distribution')\n",
    "plt.xlabel('Probability')\n",
    "plt.ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Reds):\n",
    "    \"\"\"Plots confusion matrix in red colormap\"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap = cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prints confusion matrix and precision and recall \n",
    "cm1 = confusion_matrix(y_test, tune_pred)\n",
    "\n",
    "print(cm1)\n",
    "print('\\n')\n",
    "print(\"Precision: %0.2f\" %(cm1[1, 1] / (cm1[1, 1] + cm1[0, 1])))\n",
    "print(\"Recall:    %0.2f\"% (cm1[1, 1] / (cm1[1, 1] + cm1[1, 0])))\n",
    "    \n",
    "cm2 = confusion_matrix(y_test, tune_pred, labels=[0, 1])\n",
    "    \n",
    "plt.figure()\n",
    "plot_confusion_matrix(cm2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tune model classification report, another way of visualizing metrics\n",
    "print(classification_report(y_test, tune_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the tuned model to visualize the auc, roc\n",
    "fpr, tpr,_ = roc_curve(y_test, tune_pred)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.figure()\n",
    "plt.plot([0,1],[0,1], linestyle='--', color = 'black')\n",
    "plt.plot(fpr, tpr, color = 'green')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.gca().set_aspect('equal', adjustable='box')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# creating the submission file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensuring bad features are removed from test set before training also ensures equal amount of columns\n",
    "log_test = df_test.drop(['last_five_up', 'ratio_last_five_up'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# binary classification with threshold of .5\n",
    "logit_final_pred = tune_lr.predict(log_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log odds from model\n",
    "logit_final_prob = tune_lr.predict_proba(log_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input into df to save results\n",
    "df_test['logit_predicted'] = logit_final_pred.astype('uint8')\n",
    "df_test['logit_probability'] = logit_final_prob[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create df with predicted and probability values, order size will help calculate size of the predicted order \n",
    "lr_fin = df_test.reset_index()\n",
    "lr_fin = lr_fin[['user_id', 'product_id', 'logit_predicted', 'logit_probability']]\n",
    "lr_fin.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regain order id for test set\n",
    "orders_test = orders.loc[orders.eval_set == 'test', ['user_id', 'order_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final df to calculate submission file\n",
    "lr_fin = lr_fin.merge(orders_test, on = 'user_id', how = 'left')\n",
    "lr_fin.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# threshold has been tuned increased to .7 and decreased below 0.5; 0.58 delivers the best F1 score\n",
    "d = dict()\n",
    "for row in lr_fin.itertuples():\n",
    "    if row.logit_probability >= 0.58:\n",
    "        try:\n",
    "            d[row.order_id] += ' ' + str(row.product_id)\n",
    "        except:\n",
    "            d[row.order_id] = str(row.product_id)\n",
    "\n",
    "for order in lr_fin.order_id:\n",
    "    if order not in d:\n",
    "        d[order] = 'None'\n",
    "        \n",
    "# inspect dictionary \n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dictionary into a DataFrame\n",
    "sub = pd.DataFrame.from_dict(d, orient='index')\n",
    "\n",
    "# Reset index\n",
    "sub.reset_index(inplace=True)\n",
    "# Set column names\n",
    "sub.columns = ['order_id', 'products']\n",
    "\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv('logitfinal_submission.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
